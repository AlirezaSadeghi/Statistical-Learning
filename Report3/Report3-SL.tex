\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\title{Statistical Machine Learning \\ Reading Assignment 3 Report}

\linespread{1.25}

\newtheorem{prop}{property}

\author{Alireza Sadeghi - Seyed Mohsen Shojaee}

\begin{document}

\maketitle

\section{Introduction}
The Bayesian approach to statistical problems has yielded very promising results. However in the context
of nonparametric problem is has been quite unsuccessful. One reason for this problem is the unsatisfied need for a manageable prior.
There are two essential properties for a prior distribution to work well in a nonparametric setting.
The Bayesian approach to statistical problems has yielded very promising results. However in the context
of nonparametric problem is has been quite unsuccessful. One reason for this problem is the unsatisfied need for a manageable prior.
There are two essential properties for a prior distribution to work well in a nonparametric setting.
\begin{itemize}
  \item The support of prior should be large to allow for many possible posteriors.
\item Posterior on observed data should be manageable analytically.
\end{itemize}
Dirichlet distribution introduced in \cite{fegusen} a prior
(probably the first one) for which the two properties above are simultaneously satisfied.

There are two alternate ways to define Dirichlet process. One by
describing the distribution on any finite subset and rely on
Kolmogorov's consistency theorem for the existence of the process.
The second way is by generalizing Dirichlet distribution to infinite dimension.

In this section, we first review the Dirichlet distribution. Then we provide two formal definitions and finally
give some insights about capabilities and shortcomings of Dirichlet Process i.e. the problems it can and can not model.

\subsection{Dirichlet Distribution}
A k-dimensional Dirichlet distribution is essentially a distribution over all probability measures on k possible outcomes.
For Example a 6-dimensional Dirichlet can be used to model the probability distribution of dices.
The formal definition is as follows.
Let $X_1, X_2, \ldots, \X_k$ be Gamma random variables with parameter $(\alpha_i, 1)$. A Dirichlet distribution with
parameter 
$(\alpha_1, \ldots, \alpha_n)$  is defined as distribution of $ Y = (Y_1, \ldots, Y_n)$, where
\begin{equation}
  Y_i = \frac{Z_i}{\sum_{i=1}^k Z_j}
\end{equation}
As can be seen in the above equation, $Y$ will lie on the $k-$simplex;
thus interpreted as a probability distribution over a sample space with $k$ members.

\subsection{Dirichlet Process}
We now turn to definition of Dirichlet process. Let $ \mathcal{X}$  be a measurable space
and $ \mathcal{X} $  a $\sigma-$field on it. The process is defined by specifying the joint distribution of any
Sequence $A_1, \ldots, A_n \in \mathcal{A}$.

To this end, it is sufficient to specify the distribution on every measurable portioning $B_1, \ldots, B_n$.
(We say portioning $(B_1, \ldots, B_n)$ to be measurable if $\forall i,j: B_i \in \mathcal{A}$ and $B_i \cap \B_j = \Phi$ and $\union_{j=1}^n B_j = \mathcal{X}$ )
From these distributions the distribution for arbitrary sequence $A_1, \ldots, A_n \in \mathcal{A}$ can be uniquely determined.
We are almost ready to give a formal definition for Dirichlet distribution, it only remains
the parameter of the process which should be a non-null finite measure on $\mathcal{X}$. We use $\alpha$ to denote the parameter.

We say a random process $Q$ is a Dirichlet process with parameter $\alpha$ if for every finite measurable portioning $B_1, \ldots, B_n$
the distribution of $(P(B_1), \ldots, P(B_n))$ is  Dirichlet with parameter $(\alpha(B_1), \ldots, \alpha(B_n))$.

An interesting property of Dirichlet distribution that can be considered a version of large support desired property is stated below.

Let $Q$ be a Dirichlet distribution, then for every fixed probability measure $T$ on $\mathcal{X}$ sequence of measurable sets
$A_1, \ldots, A_n \in \mathcal{A}$ and for every $\epsilon > 0$:
\begin{equation}
  \mathbb{P}(|P(A_i) - T(A_i) <\epsilon) > 0
\end{equation}

The following theorem ensures the posterior manageability for the Dirichlet process too,

\textbf{Therom 1} Let $P$ be a Dirichlet process on $(\mathcal{X},\mathcal{A})$ with parameter $\alpha $ and let $X_1,\ldots, X_n$
be a sample of size $n$ from $P$.
Then conditional distribution of $P$ given $ X_1, \ldots , X_n$, is a Dirichlet process with parameter $\alpha + \sum_{1}^n \delta_{X_i}$.




\section{Dirichlet Process}

\section{Sampling}

Let $y$ denote the observations or data, and let $\theta$ denote the parameter or set of parameters by which the data are to be summarized. Bayesian methods combine prior evidence on the parameters contained in the density $p(\theta)$ with the likelihood $p(y|\theta)$ to produce the entire posterior density $p(\theta|y)$ of $p(\theta)$.

From the posterior density one may extract any information, not simply "the most likely value" of a parameter, as with maximum likelihood (ML) estimators. However, until the advent of Monte Carlo Markov Chain methods it was not straightforward to sample from the posterior density, except in cases where it was analytically defined. Monte Carlo Markov Chain (MCMC) methods are iterative sampling methods that allow sampling from the posterior distribution, meaning $p(\theta|y)$.

Although MCMC methods can be encompassed within the broad class of \textit{Monte Carlo} methods, they must be distinguished from conventional Monte Carlo methods that generate independent simulations $\{ u_(1), u_(2), \ldots u_(T) \}$ from a target density $\pi(u)$. From such simulations the expectation of a function $g(u)$ under $\pi(u)$, namely 
    $$ \mathbb{E}_{\pi}[g(u)] = \int{g(u)\pi(u)du}$$ 
    is estimated by taking the average of the function $g(y)$ evaluated at the sampled $u^{t}$, namely 
    $$ \mathbb{E} = \sum_{t=1}^{T} g(u^{(t)})/T $$

Under independent sampling the Monte Carlo estimator \textit{g} tends to $\mathbb{E}_{\pi}[g(u)]$ as $T \rightarrow \infty$.
However, suppose we were to take $\pi(\theta) = p(\theta|y)$ as the density we wanted to find expectation from. We cannot use conventional independent Monte Carlo sampling, as this form of sampling from a posterior density $p(\theta|y)$ is not usually feasible.

When suitably implemented, MCMC methods offer an effective way to generate samples from the joint posterior distribution, $p(\theta|y)$. The "target density" for MCMC samples is the posterior density $\pi(\theta) = p(\theta|y)$, and MCMC sampling is specially relevant when the posterior cannot be stated exactly in analytic form, e.g. when the prior density assumed for $\theta$ is not conjugate with the likelihood $p(y|\theta)$.

\subsection{Forward Sampling in Bayesian Networks}
    Our goal in forward sampling is to compute the marginalized probability distribution over the desired random variable. So our goal is to compute terms of the form $p(Y=y)$. In order to do so, we generate samples from the Bayesian Network and then compute the fraction in which $Y=y$.
    
    Sampling from a Bayesian Network is done in a top-down manner. We start from the root nodes (nodes that have no parents associated with them), and sample each variable from a node's CPD (conditional probability distribution). We then move down the Bayesian Network, sampling subsequent nodes, with respect to the previously sampled parents. Upon getting to the bottom of the graphical model, we have successfully sampled a full realization of the join distribution of parameters. This process can be repeated to generate more samples and thus arrive at more accurate estimates of our favored distribution.

    With a little help from the discipline of \textit{Learning Theory}, we can introduce several interesting bounds on the number of samples, required to estimate the value of a parameter, while satisfying specific precision requirements. Two of the most important bounds are:
    
    \begin{itemize}
        \item \textit{Hoeffding's Bound (Additive)}
            \begin{equation}
                P_D (T_D \notin [p-\epsilon, p+\epsilon]) \leq 2e^{-2M\epsilon^2}                
            \end{equation}
            \begin{equation}
                M \geq \frac{\ln(2/\delta)}{2\epsilon^2}  
            \end{equation}
        \item \textit{Chernoff's Bound (Multiplicative)}
            \begin{equation}
                P_D (T_D \notin [p(1-\epsilon), p(1+\epsilon)]) \leq 2e^{-Mp\epsilon^2/3}                                        
            \end{equation}
            \begin{equation}
                M \geq 3\frac{\ln(2/\delta)}{p\epsilon^2}                  
            \end{equation}            
    \end{itemize}
    
    Where in both bounds, $M$ is the number of samples, $\delta$ is equal to the right hand side of the (1) and (3) inequalities and $T_D$ is the estimator for the parameter.


\subsection{Rejection Sampling in Bayesian Networks}
    Our goal in rejection sampling is to compute the marginalized probability distribution over a desired random variable, while taking into account and observed evidence, so our goal actually is to compute terms of the form $P(Y=y | E=e)$.

    When we have some evidence and we want to do queries with respect to those evidences, we need to take a slightly different approach. The approach we take in this configuration of the problem is called \textit{Rejection Sampling}.
    
    In rejection sampling, we start by sampling the Bayesian Network exactly as we did in normal forward sampling, and finish by computing the fraction where $Y=y$. The only difference is that we throw away all those samples that conflicts our observed evidence.
    
    As obvious, in this case more computational power is required since the number of required samples grows exponentially with the number of observed variables. (since we're keeping a sample with only probability p(e)).

\subsection{Markov Chain}

A Markov chain defines a probabilistic transition model $T(x \rightarrow x')$ over states while satisfying the following property:
$$ \forall x: \sum_{x'} T(x \rightarrow x') = 1$$

\subsubsection{Markov Property}
    For each $n \geq 1$, if $A$ is an event depending only on any subset of $\{ X_{n-1}, X_{n-2}, \ldots, 0\}$, then, for any states i and j in S,
    $$ P(X_{n+1}=j|X_n=i \quad and \quad A) = P(X_{n+1} = j| X_n=i)$$
    More generally, for each $n \geq 1$ and $m \geq 1$, if A is as defined previously, then for any states i and j in S:
    $$ P(X_{n+m}=j|X_n=i \quad and \quad A) = P(X_{n+m}=j|X_n=i)$$

\subsubsection{Temporal Dynamics}
    The probability of being in state $x'$ at the time step $t+1$, is the sum over all states, from which one can get to $x'$, multiplied by the probability of the transition occurring, namely, 
$$ P^{(t+1)}(X^{(t+1)} = x') = \sum_x P^{(t)}(X^{(t)} = x)T(x \rightarrow x')$$

\subsubsection{Stationary Distribution}
    Ultimately, as the process evolves, the probability distribution nearly equalizes. Meaning: $$ P^(t) (x') = p^(t+1)(x') = \sum_x P^{(t)}(x)T(x \rightarrow x')$$
    So, we introduce a notion of a \textit{Stationary Distribution} as follows $$ \pi(x') = \sum_x \pi(x)T(x \rightarrow x')$$
    But not all Markov Chains converge to a stationary distribution, actually only \textit{Regular Markov Chains} will eventually arrive at a stationary distribution. We define a Markov chain to be \textit{Regular} in case there exists a k, such that, for every x, x', the probability of getting from x to x' in exactly k steps is a $ > 0$. If this condition holds, we know that the Markov chain converges to a unique stationary distribution regardless of the start state. A Markov chain is regular if both the following conditions hold (Sufficient conditions):
    
    \begin{itemize}
        \item Every two states are connected
        \item For every state, there exists a self-transition
    \end{itemize}
            
\subsubsection{MCMC}
    Our goal using sampling, as explained before is to find the probability distribution $P(X \in S)$ when P is too hard to be compute analytically or to sample directly from. In this case we construct a Markov chain, whose unique stationary distribution is P. Since we only want to use samples that are sampled from a distribution close to P, we only start collecting samples after the chain has run long enough to be \textit{mixing}. So in summary:
    
    \begin{itemize}
        \item For $c=1, \ldots, C$
        \begin{itemize}
            \item Sample $X^{(c, 0)}$ from $P^{(0)}$
        \end{itemize}
        \item Repeat until mixing
        \begin{itemize}
            \item For $c=1, \ldots, C$
            \begin{itemize}
                \item Generate $X^{(c, t+1)}$ from $T(x^{(c, t)} \rightarrow x')$
            \end{itemize}
            \item Compare window statistics in different chains to determine mixing (It's a heuristic method to check whether the Markov chain has started to mix)
            \item $t=t+1$
        \end{itemize}
    \end{itemize}
    
    Now we know the Markov chain is mixing (we have transitioned in the Markov chain long enough to be sure we've reached its stationary distribution) and with that, we do actual MCMC as follows:
    
    \begin{itemize}
        \item Repeat until sufficient samples are obtained
        \begin{itemize}
            \item $D = \emptyset$
            \item For $c=1, \ldots, C$
            \begin{itemize}
                \item Generate $X^{(c, t+1)}$ from $T(x^{(c, t)} \rightarrow x')$
                \item $D = D \bigcup {X^{(c, t+1)}}$
            \end{itemize}
            \item $t = t+1$
        \end{itemize}
        \item Let $D = {x[1], \ldots, x[M]}$
        \item Estimate $E_P[f] = \frac{1}{M}\sum_{m=1}^M f(x[m])$
    \end{itemize}

\subsubsection{Neal's Algorithms for MCMC}

Here comes neals stuff.        
        
\subsection{Summary}
\begin{itemize}
    \item Pros:
    \begin{itemize}
        \item Very general purpose
        \item Often easy to implement
        \item Good theoretical guarantees when $t \rightarrow \infty$
    \end{itemize}
    \item Cons:
    \begin{itemize}
        \item Lots of tunable parameters / design choices
        \item Can be quite slow to converge
        \item Difficult to tell whether it's working (detecting mixing)
    \end{itemize}
\end{itemize}

\end{document}
